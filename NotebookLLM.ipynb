{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fedecanzo/LLM-finetuning/blob/main/NotebookLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlZUbSFTboqv"
      },
      "source": [
        "#FineTuning LLAMA2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPvu8-bQqsgO"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69P_-evPjef5"
      },
      "outputs": [],
      "source": [
        "# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
        "!pip install -q accelerate peft bitsandbytes transformers trl sentencepiece wandb chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIHdRtG8kJri"
      },
      "outputs": [],
      "source": [
        "# Non dovrebbe servire, i checkpoint vengono salvati su Huggingface HUB\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTP1SLvYqro3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import shutil\n",
        "from trl import SFTTrainer\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from peft import LoraConfig, PeftModel,get_peft_model,prepare_model_for_kbit_training,PeftConfig\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "\n",
        "## Dopo l'installazione di qualche libreria si resetta il locale\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFWLqmtnmd5Y"
      },
      "outputs": [],
      "source": [
        "# CONFIG VARIABLES\n",
        "OUTPUT_DIR = \"Llama2-to-SQL\"\n",
        "\n",
        "TOKEN_WANDB = userdata.get('WANDB_TOKEN')\n",
        "TOKEN_HF    = userdata.get('HF_TOKEN')\n",
        "\n",
        "os.environ['WANDB_API_KEY']   = TOKEN_WANDB\n",
        "os.environ[\"WANDB_PROJECT\"]   = OUTPUT_DIR\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4nF_Dk-YIxE"
      },
      "outputs": [],
      "source": [
        "#Logging su Weight and Bias\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOcsQxNMyhva"
      },
      "outputs": [],
      "source": [
        "def get_prompt_format(type_format):\n",
        "\n",
        "  if type_format==\"meta-llama\":\n",
        "    return \"<s> [INST] <<SYS>> You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA. {c} <</SYS>>. Question is: {q}. [/INST] Answer is: {a} </s>\"\n",
        "  elif type_format==\"chatml\":\n",
        "    return \"<|im_start|>system You are a helpful bot, your job it to convert input question into its respective SQL command <|im_end|> <|im_start|>user Context: {c} Question: {q} <|im_end|> <|im_start|>assistant {a} \"\n",
        "  elif type_format==\"mistral\":\n",
        "    return \"<s> [INST] You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA. {c} Question is: {q}.[/INST] Answer is: {a} </s>\"\n",
        "  else:\n",
        "    raise Exception(f\"Prompt format not supported: {type_format}\")\n",
        "\n",
        "# UTILITY FUNCTION\n",
        "def get_prompt_template(c,q,a,type_format):\n",
        "  prompt = get_prompt_format(type_format)\n",
        "  return prompt.format(c=c,q=q,a=a)\n",
        "\n",
        "# Inference\n",
        "def get_prompt_template_inference(c,q,type_format):\n",
        "  if type_format==\"meta-llama\":\n",
        "    return f\"<s>[INST] <<SYS>> You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA. {c} <</SYS>>. Question is: {q}. [/INST] \"\n",
        "  elif type_format==\"chatml\":\n",
        "    return f\"<|im_start|>system You are a helpful bot, your job it to convert input question into its respective SQL command <|im_end|> <|im_start|>user Context: {c} Question: {q} <|im_end|>  \"\n",
        "  else:\n",
        "    raise Exception(f\"Prompt format not supported: {type_format}\")\n",
        "\n",
        "\n",
        "def generate_inference(model, tokenizer, input_str, device, time_track  ):\n",
        "    start = time.time()\n",
        "    input_tokenized = tokenizer(input_str, return_tensors=\"pt\", padding=True ).to(device)\n",
        "    output_to_decode = model.generate(**input_tokenized )\n",
        "    output_str = tokenizer.decode(output_to_decode[0])\n",
        "\n",
        "    if time_track:\n",
        "      print(f\"Output: {output_str}, Tempo: {time.time()-start}\")\n",
        "\n",
        "    return output_str\n",
        "\n",
        "def forward_inference(model, tokenizer, input_str, device, time_track):\n",
        "    start = time.time()\n",
        "    input_tokenized = tokenizer(input_str, return_tensors=\"pt\").to(device)\n",
        "    output_to_decode=model(**input_tokenized)\n",
        "    output_str = tokenizer.decode(output_to_decode.logits.argmax(axis=-1)[0])\n",
        "\n",
        "    if time_track:\n",
        "      print(f\"Output: {output_str}, Tempo: {time.time()-start}\")\n",
        "\n",
        "    return output_str\n",
        "\n",
        "def print_trainable_parameters(model,str_info):\n",
        "      \"\"\"\n",
        "      Prints the number of trainable parameters in the model.\n",
        "      \"\"\"\n",
        "      trainable_params = 0\n",
        "      all_param = 0\n",
        "      for _, param in model.named_parameters():\n",
        "          all_param += param.numel()\n",
        "          if param.requires_grad:\n",
        "              trainable_params += param.numel()\n",
        "      print(f\"{str_info} -- Trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qktXRyXlqvUs"
      },
      "outputs": [],
      "source": [
        "def load_model(model_id,device_map,type_quantization):\n",
        "    #LOAD MODEL\n",
        "    MODEL_NAME=model_id\n",
        "    DEVICE_MAP=device_map\n",
        "\n",
        "    if type_quantization==\"8bit\":\n",
        "      bnb_config = BitsAndBytesConfig(\n",
        "          load_in_8bit=True,\n",
        "      )\n",
        "    elif type_quantization==\"4bit\":\n",
        "      bnb_config = BitsAndBytesConfig(\n",
        "          load_in_4bit=True,\n",
        "          bnb_4bit_use_double_quant=True,\n",
        "          bnb_4bit_quant_type=\"nf4\",\n",
        "          bnb_4bit_compute_dtype=torch.bfloat16\n",
        "      )\n",
        "    elif type_quantization==None:\n",
        "      bnb_config=None\n",
        "    else:\n",
        "      raise Exception(f\"4bit o 8bit expected instead receiving: {type_quantization}\")\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map = DEVICE_MAP,\n",
        "        token = TOKEN_HF,\n",
        "        quantization_config=bnb_config,\n",
        "    )\n",
        "\n",
        "    model.config.pretraining_tp = 1\n",
        "    model.config.use_cache = False\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_tokenizer(model_id):\n",
        "  #LOAD TOKENIZER\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenizer.padding_side = \"right\"\n",
        "  return tokenizer\n",
        "\n",
        "# Aggiunge i parametri extra al modello (Ã¨ un operazione in PLACE)\n",
        "def get_trainable_model(model, peft_config):\n",
        "  model = get_peft_model(model, peft_config)\n",
        "  return model\n",
        "\n",
        "DEVICE_MAP=\"auto\"\n",
        "TYPE_QUANTIZATION=\"4bit\"\n",
        "\n",
        "MODEL_NAME=\"meta-llama/Llama-2-7b-chat-hf\"\n",
        "# MODEL_NAME=\"meta-llama/Llama-2-7b-hf\"\n",
        "# MODEL_NAME=\"teknium/OpenHermes-2.5-Mistral-7B\" #Strano\n",
        "# MODEL_NAME=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "# MODEL_NAME=\"codellama/CodeLlama-7b-hf\"\n",
        "# codellama fine tuning di llama2 su codice e instruction\n",
        "\n",
        "model = load_model(MODEL_NAME, DEVICE_MAP, TYPE_QUANTIZATION)\n",
        "tokenizer_llama = load_tokenizer(MODEL_NAME)\n",
        "\n",
        "# print_trainable_parameters(model,\"Modello normale\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model, \"Modello normale\") #0%"
      ],
      "metadata": {
        "id": "A1CAdnaVzleR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v64h7MJctH8o"
      },
      "outputs": [],
      "source": [
        "def load_dataset_from_hf(dataset_id,test_size=0.1):\n",
        "    dataset = load_dataset(dataset_id)\n",
        "    dataset = dataset['train'].train_test_split(test_size=test_size)\n",
        "    return dataset\n",
        "\n",
        "def get_dataset_with_prompt(dataset,type_format_prompt):\n",
        "  create_prompt = lambda row: {'text' : [ get_prompt_template(c,q,a,type_format_prompt) for c,q,a in zip(row[\"context\"], row['question'], row['answer'])]}\n",
        "  ddict=DatasetDict()\n",
        "  train_dataset = dataset['train'].map(create_prompt, batched=True )\n",
        "  test_dataset = dataset['test'].map(create_prompt, batched=True )\n",
        "  ddict['train']=train_dataset\n",
        "  ddict['test']=test_dataset\n",
        "  return ddict\n",
        "\n",
        "#PER TEST\n",
        "def get_reduced_train_test_dataset(dataset,reduced_size):\n",
        "    ddict=DatasetDict()\n",
        "    train_dataset = dataset['train'].select([i for i in range(reduced_size)]).shuffle(seed=42)\n",
        "    test_dataset = dataset['test'].select([i for i in range(int(reduced_size*0.25))]).shuffle(seed=42)\n",
        "    ddict['train']=train_dataset\n",
        "    ddict['test']=test_dataset\n",
        "    return ddict\n",
        "\n",
        "type_format_prompt=\"meta-llama\"\n",
        "dataset = load_dataset_from_hf(\"b-mc2/sql-create-context\", test_size=0.1)\n",
        "dataset = get_dataset_with_prompt(dataset,type_format_prompt)\n",
        "dataset = get_reduced_train_test_dataset(dataset,reduced_size=1000)\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train']['text'][0])"
      ],
      "metadata": {
        "id": "htVJTG2Yeg-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZObz5D2-hFZ"
      },
      "outputs": [],
      "source": [
        "def get_lora_config(r, alpha, dropout):\n",
        "    lora_config = LoraConfig(\n",
        "      lora_alpha=alpha,\n",
        "      lora_dropout=dropout,\n",
        "      r=r,\n",
        "      target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head'],\n",
        "      bias=\"none\",\n",
        "      task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    return lora_config\n",
        "\n",
        "LORA_ALPHA = 128\n",
        "LORA_DROPOUT= 0.1\n",
        "LORA_R = 128\n",
        "# https://arxiv.org/abs/2106.09685 LoRA paper\n",
        "peft_config = get_lora_config(LORA_R, LORA_ALPHA, LORA_DROPOUT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DikfNkwRSK9B"
      },
      "outputs": [],
      "source": [
        "model = get_trainable_model(model,peft_config)\n",
        "print_trainable_parameters(model,\"MODELLO CON PARAMETRI EXTRA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BInPRIoxdfe"
      },
      "outputs": [],
      "source": [
        "# save adapter file su gdrive (NON SERVE PIÃ¹)\n",
        "class SaveOnGDriveCallback(TrainerCallback):\n",
        "    def on_save(self, args, state, control, logs=None, **kwargs):\n",
        "        step = state.global_step\n",
        "        shutil.copytree(f\"/content/{OUTPUT_DIR}/checkpoint-{str(step)}\",f\"/content/drive/MyDrive/test-checkpoint-{str(step)}\",dirs_exist_ok = True)\n",
        "\n",
        "# Test EvalCallback\n",
        "class EvalTest(TrainerCallback):\n",
        "    def on_evaluate(self, args, state, control, logs=None, **kwargs):\n",
        "        # print(self,dir(self),kwargs)\n",
        "        # print(kwargs['model'], kwargs['tokenizer'])\n",
        "        print(kwargs['eval_dataloader'],dir(kwargs['eval_dataloader']), kwargs['eval_dataloader'].dataset )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8B1N0loqvW8"
      },
      "outputs": [],
      "source": [
        "# Set training parameters\n",
        "NUM_TRAIN_EPOCHS = 2\n",
        "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
        "# PER_DEVICE_TRAIN_BATCH_SIZE = 8\n",
        "# PER_DEVICE_TRAIN_BATCH_SIZE = 32 # Cuda OOM\n",
        "\n",
        "PER_DEVICE_EVAL_BATCH_SIZE = 4\n",
        "\n",
        "GRADIENT_ACCUMULATION_STEPS = 3\n",
        "# OPTIM = \"paged_adamw_32bit\"\n",
        "OPTIM = \"paged_adamw_8bit\"\n",
        "# OPTIM = \"adamw_torch\"\n",
        "\n",
        "SAVE_STEPS = 25\n",
        "SAVE_STRATEGY = \"steps\"\n",
        "\n",
        "EVAL_STEPS = 5\n",
        "EVAL_STRATEGY = \"steps\"\n",
        "\n",
        "LOGGING_STEPS = 1\n",
        "\n",
        "LEARNING_RATE = 2e-4\n",
        "WEIGHT_DECAY = 0.001\n",
        "LR_SCHEDULER_TYPE = \"constant\"\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "PACKING = False\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    optim=OPTIM,\n",
        "\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_strategy=SAVE_STRATEGY,\n",
        "\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "\n",
        "    evaluation_strategy=EVAL_STRATEGY,\n",
        "    eval_steps=EVAL_STEPS,\n",
        "\n",
        "    fp16=True,\n",
        "\n",
        "    max_grad_norm=0.3,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    hub_strategy = \"checkpoint\",\n",
        "    push_to_hub = True,\n",
        "    report_to=\"wandb\",\n",
        "    overwrite_output_dir = True,\n",
        ")\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=PACKING,\n",
        "\n",
        "    neftune_noise_alpha=5,\n",
        "    # callbacks=[EvalTest],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(max([ len(i) for i in dataset['train']['text']]))\n",
        "print(max([ len(i) for i in dataset['test']['text']]))"
      ],
      "metadata": {
        "id": "ZzV4m6AHmJjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOMz3YIWqvcM"
      },
      "outputs": [],
      "source": [
        "# trainer.train(resume_from_checkpoint=False)\n",
        "trainer.train(resume_from_checkpoint=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.eval()\n",
        "number_of_eval_samples = 10\n",
        "equal=0\n",
        "infer= []\n",
        "for record in tqdm(dataset['test'].shuffle().select(range(number_of_eval_samples))):\n",
        "  prompt_infer = get_prompt_template_inference(c=record['context'],q=record['question'],type_format=\"meta-llama\")\n",
        "  # success_rate.append(evaluate(prompt_infer, trainer.model, tokenizer, \"cuda:0\"))\n",
        "  input_str=prompt_infer\n",
        "  input_tokenized = tokenizer(input_str, return_tensors=\"pt\",padding=True,add_special_tokens=False).to(\"cuda:0\")\n",
        "\n",
        "  output_to_decode = model.generate( **input_tokenized, do_sample=True, max_new_tokens = 64 )\n",
        "  output_str = tokenizer.decode(output_to_decode[0])\n",
        "  output_label = get_prompt_template(c=record['context'],q=record['question'],type_format=\"meta-llama\", a=record['answer'])\n",
        "  print(\"Input: \", input_str ,\"\\nInference: \", output_str, \"\\nGroundTruth: \", output_label,\"\\n######\\n\" )\n",
        "  infer.append([output_str,output_label])\n",
        "  if output_str.strip() == output_label.strip():\n",
        "    equal+=1\n",
        "\n",
        "print(\"Accuracy: \", equal/len(dataset['test']))"
      ],
      "metadata": {
        "id": "ruKu8UaxMCYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install sentence_transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer,util\n",
        "\n",
        "# Load a sentence embedding model\n",
        "model_sentence = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "a =  \"\"\"SELECT source FROM table_name_62 WHERE cartridge = \".375 remington ultra magnum\" \"\"\"\n",
        "b =  \"\"\"SELECT source FROM table_name_62 WHERE cartridge = \".325 remington ultra magnum\" </s>\"\"\"\n",
        "\n",
        "a_vector = model_sentence.encode(a)\n",
        "b_vector = model_sentence.encode(b)\n",
        "\n",
        "# Calculate semantic similarity\n",
        "# similarity = model.similarity(apple_sentence, orange_sentence)\n",
        "print(util.pytorch_cos_sim(a_vector, b_vector))\n",
        "print(util.cos_sim(a_vector, b_vector))\n",
        "print(util.semantic_search(a_vector, b_vector))"
      ],
      "metadata": {
        "id": "6UfWeO02qfei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD0gh4_M3cIM"
      },
      "outputs": [],
      "source": [
        "\n",
        "context=\"CREATE TABLE table_name_35 (mountain_range VARCHAR, rank VARCHAR)\"\n",
        "question=\"Which mountain range has a rank of 200?\"\n",
        "type_format=\"mistral\"\n",
        "device=\"cuda:0\"\n",
        "\n",
        "# trainer.model.eval()\n",
        "#\n",
        "print(generate_inference(trainer.model, tokenizer, get_prompt_template_inference(c=context, q=question,type_format=type_format),device,True))\n",
        "# print(forward_inference(merged_model, tokenizer, get_prompt_template_inference(c=context, q=question),\"cuda:0\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ju69PGO8aMn"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "generator=pipeline(task=\"text-generation\",model=model, tokenizer=tokenizer )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUV0aJNu7QPN"
      },
      "outputs": [],
      "source": [
        "#pulizia vedere se svuota la VRAM (se non funziona devi riavviare il noteboook)\n",
        "import gc\n",
        "# del trainer\n",
        "# del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhYUtFXkM5Qn"
      },
      "outputs": [],
      "source": [
        "# Test carica un dataset da JSON e push su HUB\n",
        "\n",
        "from datasets import load_dataset\n",
        "datasets = load_dataset(\"json\", data_files=\"dataset.json\", field=\"data\")\n",
        "\n",
        "datasets.push_to_hub(\"test_dataset-Private\",private=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pywhd1j4Mr7v"
      },
      "outputs": [],
      "source": [
        "#Carica sia modello che adapter ()\n",
        "\n",
        "def load_model_and_adapter_from_hf(base_id, adapter_id):\n",
        "  base_model = AutoModelForCausalLM.from_pretrained(base_id,token=TOKEN_HF,load_in_8bit=True)\n",
        "  peft_model = PeftModel.from_pretrained(base_model, adapter_id )\n",
        "  return peft_model\n",
        "\n",
        "adapter_id=\"Federic/lora-fine-tuning-llama2-SQL-lora-100-dataset-size\"\n",
        "base_id =\"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "peft_model=load_model_and_adapter_from_hf(base_id, adapter_id)\n",
        "tokenizer = load_tokenizer(base_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test ChromaDB"
      ],
      "metadata": {
        "id": "-gDBi1yXnesk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "biSaHekWng9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "chroma_client = chromadb.Client()"
      ],
      "metadata": {
        "id": "cM6BxZNznszm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection = chroma_client.create_collection(name=\"external_documents\")"
      ],
      "metadata": {
        "id": "Yjq_iHlUns2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records = []\n",
        "records.append((\"How many heads of the departments are older than 56 ?\t\", \"SELECT COUNT(*) FROM head WHERE age > 56\", \"CREATE TABLE head (age INTEGER)\"))\n",
        "records.append((\"List the name, born state and age of the heads of departments ordered by age.\", \"SELECT name, born_state, age FROM head ORDER BY age\", \"CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR)\"))\n",
        "records.append((\"List the creation year, name and budget of each department.\",\"SELECT creation, name, budget_in_billions FROM department\", \"CREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR)\"))\n",
        "records.append((\"What are the maximum and minimum budget of the departments?\", \"SELECT MAX(budget_in_billions), MIN(budget_in_billions) FROM department\", \"CREATE TABLE department (budget_in_billions INTEGER)\"))\n",
        "\n",
        "questions = [ rec[0] for rec in records]\n",
        "sqls =      [ rec[1] for rec in records]\n",
        "ddls =      [ rec[2] for rec in records]\n"
      ],
      "metadata": {
        "id": "D34r7PUhns49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection.add(\n",
        "    documents=questions,\n",
        "    metadatas=[{\"source\": \"question\"} for i in range(len(questions)) ],\n",
        "    ids=[\"question-\"+str(i+1) for i in range(len(questions))]\n",
        ")\n",
        "\n",
        "collection.add(\n",
        "    documents=ddls,\n",
        "    metadatas=[{\"source\": \"ddl\"} for i in range(len(ddls)) ],\n",
        "    ids=[\"ddl-\"+str(i+1) for i in range(len(ddls))],\n",
        ")\n",
        "\n",
        "collection.add(\n",
        "    documents=sqls,\n",
        "    metadatas=[{\"source\": \"sql\"} for i in range(len(sqls)) ],\n",
        "    ids=[\"sql-\"+str(i+1) for i in range(len(sqls))]\n",
        ")\n"
      ],
      "metadata": {
        "id": "7PcZfN9Dns7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prendo le n domande piÃ¹ simili\n",
        "def get_relevant_document(prompt,n_results):\n",
        "  res = collection.query(\n",
        "    query_texts = [prompt],\n",
        "    n_results=n_results,\n",
        "    where = {\n",
        "        \"source\": {\n",
        "              \"$eq\": \"question\"\n",
        "          }\n",
        "      }\n",
        "  )\n",
        "\n",
        "  sqls = [collection.get( str(\"sql-\")+i.split(\"question-\")[1])['documents'] for i in res['ids'][0]]\n",
        "  return list(zip(res['documents'][0], sqls))\n",
        "\n",
        "def get_prev_docs(docs):\n",
        "  out=\"\"\n",
        "  for input,output in docs:\n",
        "    out+=f\"Input: {input}, Output: {output[0]} -- \"\n",
        "\n",
        "  return out+ \".\"\n"
      ],
      "metadata": {
        "id": "-kZzMJ8mvz4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collection.delete(collection.get()['ids'])\n",
        "# collection.get()['ids']\n",
        "#\n",
        "#"
      ],
      "metadata": {
        "id": "Hg68lT6HrSZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type_format=\"meta-llama\"\n",
        "q=\"What are the distinct ages of the heads who are acting?\"\n",
        "docs=get_relevant_document(q,2)\n",
        "previous_docs = f\"Here some similar couple (input, output): {get_prev_docs(docs)} \"\n",
        "c=f\"CREATE TABLE head (age VARCHAR, head_id VARCHAR); CREATE TABLE management (head_id VARCHAR, temporary_acting VARCHAR) -- {previous_docs} \"\n",
        "prompt=get_prompt_template_inference(c=c, q=q, type_format=type_format)\n",
        "print(prompt)\n"
      ],
      "metadata": {
        "id": "3ooUtlCd4CHy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "LlZUbSFTboqv",
        "dMdIgj-E_8Zd",
        "-gDBi1yXnesk",
        "yRS6VGXFJQvH",
        "1lPiURY8Ht3I"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}